<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Modeling Basics and Statistical Bias | </title>
    <meta name="author" content="Harpreet  Matharoo">
    <meta name="description" content="A study of statistical bias for modeling a simple prediction problem.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2023/modeling-basics/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"></a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Modeling Basics and Statistical Bias</h1>
    <p class="post-meta">May 10, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/theory">
          <i class="fas fa-hashtag fa-sm"></i> Theory</a>  
          <a href="/blog/tag/probability">
          <i class="fas fa-hashtag fa-sm"></i> Probability</a>  
          <a href="/blog/tag/modeling">
          <i class="fas fa-hashtag fa-sm"></i> Modeling</a>  
          
        ·  
        <a href="/blog/category/fundamentals">
          <i class="fas fa-tag fa-sm"></i> Fundamentals</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p><em>“All models are wrong, but some are useful” - George Box</em></p>

<h2 id="introduction">Introduction</h2>

<p>Following our discussion on mind-projection fallacy in my previous article, I want to now talk about what happens when we model a simple scenario. In this article, I take you through a fictitious example of modelling a prediction problem. We are given a coin and we are tasked to estimate the probability that it will fall on heads upon tossing. We have no idea how this coin behaves, but we might discover something advantageous by playing with it. We look to statistical learning methods to guide our discovery.</p>

<p>In the next section, I discuss what kind of models are appropriate to estimate the probability. To this end, we discuss two simple models with different capacity. The subsequent section presents the experiments I performed to contrast these models. I consider two different data generating processes as an approximation of the game to collect the data, which is use later to fit the models. I then present the results of these experiments, followed by a discussion contrasting the applicability of these models and our findings.</p>

<h2 id="methodology">Methodology</h2>

<p>First and foremost, we need a model to represent the behaviour of the coin. More specifically, we are interested in capturing certain aspects of the data-generating process. You may recall that the process of repeated coin toss is also known as Bernoulli trials. Here, we assume each coin toss is independent of the other and the coin has a probability p with the heads as the outcome of a toss. Thus, we can estimate p by performing a number of trials and computing the value that maximizes the probability. Before we go further, we need to emphasize the concept of probability here. The probability here represents a degree of plausibility that we measure in the range of 0 and 1. It is important to note this degree of plausibility is entirely a mathematical tool and may not represent reality itself. After all, the outcome of a coin toss is either heads or tails. Perhaps, we could accurately predict the exact outcome of the toss by running a multi-physics simulation that models the dynamics of the coin toss in the presence of air resistance and gravity. However, these efforts might be too complex to realize. We instead start with a simpler model based on our intuition of the data-generating process.</p>

<p>Now, we start by generating samples. Suppose we perform \(N\) trials out of which we get m heads and n tails. It can be shown the fraction of trial for which the coin fell on heads is the maximum likelihood estimated(MLE) value of \(p\). In layman’s terms, this is the value of \(p\) that maximizes the plausibility of observing the outcomes we did given the data model. To proceed further, let us first go through how the above computation unfolds to make an MLE of \(p\). We represent the outcome of trial \(i\) as \(X_i\). The probability that we observe such outcomes condition on the value of \(p\)  is:</p>

\[P(X_1, X_2, .. X_n|p) = \prod_i^n P(X_i|p)\]

<p>Notice the right-hand side decomposes the conditional events. This is valid under the assumption the trials are independently distributed as we noted previously. Now expanding the right-hand side part of the equation is easy:</p>

\[\prod_i^n P(X_i|p) = p^m(1-p)^{n-m}\]

\[\cfrac{\partial P(X_1, X_2, .. X_n|p)}{\partial p} \Biggm\lvert_{p_{MLE}}= \cfrac{\partial p^m(1-p)^{n-m}}{\partial p} \Biggm\lvert_{p_{MLE}}= 0\]

\[\Rightarrow p_{MLE} = \cfrac{m}{m+n}\]

<p>It is customary to consider several models for comparison. To this end, we consider a slight variation of the Bernoulli trials. We lift the condition of independence between trials. We chose a model with Markov property between subsequent trials. If the coins is facing heads up, it is likely to come up with heads by probability \(p + \delta\), otherwise it is \(p - \delta\). The figure below illustrates the difference between the state diagrams of the model without and with Markov property.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/modelingbasics/state_diagram-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/modelingbasics/state_diagram-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/modelingbasics/state_diagram-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modelingbasics/state_diagram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1. State Diagram of the Bernoulli trials with and without Markov property.
</div>

<p>We are now required to estimate both the parameters in case of the model with Markov property. Note, the one with Markov property is generalization of the one without. By setting $\delta=0$, we recover Bernoulli trial. Now, just like in the case of Bernoulli trials, we wish to estimate the maximum likelihood values of both \(\delta\) and \(p\). The likelihood is formulated as follows:</p>

\[P(X_1, X_2, .. X_n|p) = P(X_o|p, \delta)\prod_i^n P(X_i|p, X_{i-1})\]

<p>We divide our sequence of trials into pairs of subsequent trials to reduce the above formulation further. We can have four types of such pairs - \(HH\), \(HT\), \(TH\), and \(TT\). Suppose we have \(m\), \(n\), \(r\) and \(s\) number of occurrences of such pairs. Then the above formulation reduces to the following expression:</p>

\[P(X_1, X_2, .. X_n|p) = P(X_o|p, \delta) (p \ + \delta)^m(p \ - \delta)^r(1 - (p \ + \delta))^n(1 - (p \ - \delta))^s\]

<p>We can then compute the formula for the MLE values using calculus just like we did before. Given the complexity of the model, we will estimate these values numerically instead.</p>

<h2 id="experiments">Experiments</h2>

<p>We perform four experiments in total. We generate two types of datasets assuming Bernoulli trials without Markov property and with Markov property. We simulated these datasets by assuming \(p = 0.7\) and \(\delta = 0.1\). These values are chosen arbitrarily so that the coin is biased in both the cases and the data generated with Markov property has a more complex process than the without. We vary the number of samples per dataset from 10 to 100 in steps of 10. For each setting, we generate 100 datasets to accurately compute the mean values of \(p_{MLE}\) and \(\delta_{MLE}\).</p>

<h2 id="results-and-discussion">Results and Discussion</h2>

<p>The results of our experiments are plotted in the figures below. Figure 2 presents \(p_{MLE}\) computed for the data generated with Bernoulli coin. Dashed line represents the true value of p, solid lines represent the mean values of p estimated by Bernoulli and Markov model, and the shaded region represents the standard deviation for both the models. Notice both models are able to accurately estimate the true value of p as indicated by the proximity of the mean value to the true value. However, the estimation by the Markov model has higher variance than the Bernoulli model. This is because the Bernoulli model has fewer excess parameters to capture the underlying data generation process. On the other hand, Markov model has the parameter \(\delta\) which may overfit to the superficial irregularities causing the estimated p to have higher variance. Figure 4 further corroborate our claim. Notice the model fits a non-zero value to \(\delta\) for all experiments.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;" text-align="center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/modelingbasics/bernoulli_data-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/modelingbasics/bernoulli_data-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/modelingbasics/bernoulli_data-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modelingbasics/bernoulli_data.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2. Maximum Likelihood Estimate of p for Bernoulli Coin with different models
</div>
<p>Figure 3 presents the results of fitting our models to the data generated for the Markov coin. Since Bernoulli model assumes \(\delta = 0\), it fails to account for the Markov property and ends up assigning a higher value to \(p\) than its true value. Markov model on the other hand is able to capture the true value of \(p\) accounting for the Markov property. We can see in Figure 4 that the model accurately predicts the value of \(\delta\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;" text-align="center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/modelingbasics/markov_data-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/modelingbasics/markov_data-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/modelingbasics/markov_data-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modelingbasics/markov_data.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3. Maximum Likelihood Estimate of p for Markov Coin with different models
</div>

<p>Finally, note the spread in all figures reduce as we increase the number of samples per dataset. This is aligned with our expectations as larger dataset allows the model to capture more signal, which in turn increases its precision.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;" text-align="center">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/modelingbasics/delta-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/modelingbasics/delta-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/modelingbasics/delta-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/modelingbasics/delta.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 4. Maximum Likelihood Estimate of delta for both coins with Markov Model
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In this article, we studied the basics of modelling by considering two toy datasets. The datasets are generated by a biased coin with and without Markov property. We fitted two models with varying capacities to the datasets generated for these processes. We chose varying capacities to demonstrate the sample complexity of training them. The training objective is to estimate the probability of getting a head for a coin toss. We found Bernoulli Model(the one with lower capacity) requires fewer samples than Markov model(more capacious) to precisely estimate the probability of the Bernoulli coin. Both models are equally accurate. However, as we increase the number of samples per dataset, both models become more precise. On the other hand, the higher capacity of the Markov model allows it to accurately estimate the probability of the Markov coin, while Bernoulli model fails to capture the effect of the previous state. We argue the difference in this performance is because Markov model has more capacity thus it can overfit to superficial irregularities. It requires more samples to ignore them. On the other hand, simpler models like Bernoulli can easily ignore them as they assume their non-existence. However, models can be inaccurate if it fails to account for some aspects of the data generation process. This behaviour is clearly seen for the Markov coin.</p>

<p>This phenomenon of how the alignment between the model and the data generating process affect the model accuracy and precision is a classic case of statistical bias-variance trade-off[1]. It is important to note the discussion on bias-variance trade-off in popular machine learning literature on the web attributes the difference in the prediction and the ground truth to the capacity of the model. However, this argument is incomplete as we must take the data generating process into account. A more capacious model does not have to exhibit low bias. Also, modern machine learning theory tells us more capacious model can increase both the accuracy and the precision[2]. Finally, statistical bias is just a part of the story. There are other forms of bias present in machine learning algorithms[3] that I intend to cover in one of the subsequent articles.</p>

<h2 id="references">References</h2>
<p>[1] Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.<br>
[2] Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32), 15849-15854.<br>
[3] Mitchell, T. M. (2007). Machine learning (Vol. 1). New York: McGraw-hill.</p>

  </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "mathadoor/mathadoor.github.io",
        "data-repo-id": "R_kgDOIvjyVQ",
        "data-category": "General",
        "data-category-id": "DIC_kwDOIvjyVc4CU80N",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Harpreet  Matharoo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
